- 学习率 :
  直接以导数值的大小作为步长看起来不错，但是实际上还需要乘以一个步长的系数。步长系数更正式的名字叫做学习率（ Learning Rate）。简写做 lr。因为导数值并不是总是那么合适，有时候偏大，导致步长太大，跳过全局最小值。有时候偏小，导致训练过程很慢。这时就可以通过设置学习率来调整。一般情况下，学习率都是小于 1 的。比如设置为 0.001 。

- 参数和超参数：
  在机器学习领域，在训练过程中由算法调整的变量叫做参数。参数是通过数据学习而来的，不是人为设定的。参数决定了模型如何从输入数据映射到输出。训练好的模型会保存这些参数，以便在预测 / 推理时使用。比如线性回归里的权重和偏置就是参数。
  超参数是模型训练前需要人为设置的变量，它们不会在训练过程中自动学习，而是由人根据实验或者经验设定的。比如学习率就是一个超参数。

  - 与一元函数求最小值类似，我们可以给每个参数随机设置初始值 (w0, b0)。根据梯度的定义我们知道，沿梯度方向 $[ \frac{ \partial f}{ \partial w} \left( w_{0},b_{0} \right), \frac{ \partial f}{ \partial b} \left( w_{0},b_{0} \right)]$ 函数值增长最快，那么沿着梯度的负方向，函数值就下降最快。所以我们选择沿着梯度的负方向，给梯度值乘以学习率 lr 作为步长前进。

$$
\begin{array}{c}{w_{0}= w_{0}-lr\frac{\partial f}{\partial w}(w_{0}, b_{0}) }\\ \\{b_{0}= b_{0}-lr\frac{\partial f}{\partial b}(w_{0}, b_{0})}\end{array}
$$
这样不断迭代，最终得到 loss 最小时的 w 和 b。

#### 梯度下降算法的几个要素
##### 初始化
对于每个要优化的参数，随机初始化一个值。
##### 计算梯度
根据当前参数值，对每个参数求偏导，得到梯度值。
##### 迭代更新参数
让当前参数向量沿着梯度负方向，用学习率乘以梯度的值前进。然后再计算梯度。
##### 停止条件
可以设置迭代次数，当到达迭代次数后则停止更新。
或者可以设置 loss 连续多个迭代不减小，则认为到达 loss 最小处，停止更新。

#### 损失函数除以样本数
每个参数在每次梯度下降迭代时，都和损失函数的偏导数相关。损失函数不能和样本个数相关。之前我们定义的损失函数是所有样本的 label 和预测值的误差的平方和。

为了让训练稳定，不同的样本数也有差不多大小的偏导数值。一般在 loss 函数都会除以计算 loss 的样本数。

对于回归问题，一般采用的均方误差（ Mean Squared Error， MSE）。其公式为：
$$
MSE = \frac{1}{n} \sum_{i = 1} ^ {n} \left( y_{i}- \widehat{y}_{i} \right) ^ {2}
$$